# --- ETAPA 1: CARREGAMENTO E PR칄-PROCESSAMENTO ---
import pandas as pd
import numpy as np
import os

# Pega o diret칩rio do seu script atual
current_directory = os.path.dirname(__file__)

# Constr칩i o caminho para o arquivo de entrada
input_file_path = os.path.join(current_directory, 'banco_de_dados', 'database_reduced.csv')

# Carregar seus dados
data = pd.read_csv(input_file_path)

# Fun칞칚o para converter dB para linear
def db_to_linear(s11_db):
    return 10 ** (s11_db / 20)

# Converter cada coluna S11 (dB) para absor칞칚o
for i in [1, 2, 3]:
    s11_db_col = f'S11_{i}'
    abs_col = f'Absorption_{i}'
    s11_linear = db_to_linear(data[s11_db_col])
    data[abs_col] = 1 - np.abs(s11_linear) ** 2

# Constr칩i o caminho para o arquivo de sa칤da
output_file_path = os.path.join(current_directory, 'banco_de_dados', 'absorcao_resultados.csv')

# Salvar os resultados intermedi치rios
data.to_csv(output_file_path, index=False)

print("Processo conclu칤do e arquivo 'absorcao_resultados.csv' salvo com sucesso!")

# --- ETAPA 2: PREPARA칂츾O PARA MODELAGEM ---

# Bibliotecas de manipula칞칚o e an치lise de dados
import random
import pandas as pd
import numpy as np

# Bibliotecas de aprendizado de m치quina
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import KernelPCA
from sklearn.pipeline import Pipeline
import xgboost as xgb

# Bibliotecas para redes neurais
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from scikeras.wrappers import KerasRegressor

# Definir a semente para reprodutibilidade
seed = 0
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# Carregar os dados processados
file_path = os.path.join(current_directory, 'banco_de_dados', 'absorcao_resultados.csv')
df = pd.read_csv(file_path)

# Remo칞칚o de outliers pelo m칠todo IQR
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR
df = df[~((df < limite_inferior) | (df > limite_superior)).any(axis=1)]

# Separar entradas (X) e sa칤das (y) - sem feature engineering
X = df[['Tx1', 'Tx2', 'Tx3', 'w']].copy()
y = df[['Absorption_1', 'Absorption_2', 'Absorption_3', 'Freq_1', 'Freq_2', 'Freq_3']]

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

# Normalizar os dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# --- ETAPA 3: OTIMIZA칂츾O E AVALIA칂츾O DOS MODELOS COM VALIDA칂츾O CRUZADA ---

cv_strategy = KFold(n_splits=5, shuffle=True, random_state=seed)
results = {}

# --- Modelo 1: XGBoost ---
print("--- Otimizando XGBoost ---")
param_grid_xgb = {
    'max_depth': [5, 10, 15],
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.05, 0.1]
}
grid_search_xgb = GridSearchCV(
    estimator=xgb.XGBRegressor(random_state=seed),
    param_grid=param_grid_xgb,
    cv=cv_strategy,
    scoring='r2',
    verbose=1,
    n_jobs=-1
)
grid_search_xgb.fit(X_train_scaled, y_train)
best_xgb = grid_search_xgb.best_estimator_
y_pred_xgb = best_xgb.predict(X_test_scaled)
results['XGBoost'] = {
    'Melhor R (CV)': grid_search_xgb.best_score_,
    'R no Teste': r2_score(y_test, y_pred_xgb),
    'Melhores Par칙metros': grid_search_xgb.best_params_
}

# --- Modelo 2: Kernel PCA + Random Forest ---
print("\n--- Otimizando Kernel PCA + Random Forest ---")
pipeline_rf = Pipeline([
    ('kpca', KernelPCA(random_state=seed)),
    ('rf', RandomForestRegressor(random_state=seed))
])
param_grid_rf = {
    'kpca__n_components': [4, 6, 8],
    'kpca__kernel': ['rbf', 'poly', 'sigmoid'],
    'rf__n_estimators': [100, 200],
    'rf__max_depth': [10, 15]
}
grid_search_rf = GridSearchCV(
    estimator=pipeline_rf,
    param_grid=param_grid_rf,
    cv=cv_strategy,
    scoring='r2',
    verbose=1,
    n_jobs=-1
)
grid_search_rf.fit(X_train_scaled, y_train)
best_rf = grid_search_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test_scaled)
results['Kernel PCA + RF'] = {
    'Melhor R (CV)': grid_search_rf.best_score_,
    'R no Teste': r2_score(y_test, y_pred_rf),
    'Melhores Par칙metros': grid_search_rf.best_params_
}

# --- Modelo 3: Rede Neural (MLP) ---
print("\n--- Otimizando Rede Neural (MLP) ---")
def create_mlp_model(neuron_count=128, dropout_rate=0.2, learning_rate=0.001):
    model = Sequential([
        Dense(neuron_count, activation='relu', input_shape=(X_train_scaled.shape[1],)),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(neuron_count // 2, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(y_train.shape[1])
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

fit_params = {
    'callbacks': [EarlyStopping(monitor='loss', patience=20, restore_best_weights=True, verbose=0)]
}

param_grid_mlp = {
    'model__neuron_count': [64, 128],
    'model__dropout_rate': [0.2, 0.3],
    'model__learning_rate': [0.001, 0.01],
    'batch_size': [16, 32],
    'epochs': [200]
}

mlp_model = KerasRegressor(model=create_mlp_model, verbose=0)

grid_search_mlp = GridSearchCV(
    estimator=mlp_model,
    param_grid=param_grid_mlp,
    cv=cv_strategy,
    scoring='r2',
    verbose=1,
    n_jobs=-1
)
grid_search_mlp.fit(X_train_scaled, y_train, **fit_params)
best_mlp = grid_search_mlp.best_estimator_
y_pred_mlp = best_mlp.predict(X_test_scaled)
results['Rede Neural (MLP)'] = {
    'Melhor R (CV)': grid_search_mlp.best_score_,
    'R no Teste': r2_score(y_test, y_pred_mlp),
    'Melhores Par칙metros': grid_search_mlp.best_params_
}


# --- ETAPA 4: COMPARA칂츾O FINAL DOS RESULTADOS ---
print("\n\n--- Tabela Comparativa de Resultados ---\n")

results_df = pd.DataFrame(results).T
results_df = results_df[['R no Teste', 'Melhor R (CV)', 'Melhores Par칙metros']]
print(results_df)

best_model_name = results_df['R no Teste'].idxmax()
print(f"\n游끥 O melhor modelo com base no R do conjunto de teste 칠: {best_model_name}")
